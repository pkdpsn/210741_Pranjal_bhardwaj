{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch .cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.96</td>\n",
       "      <td>41.76</td>\n",
       "      <td>1024.07</td>\n",
       "      <td>73.17</td>\n",
       "      <td>463.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.18</td>\n",
       "      <td>62.96</td>\n",
       "      <td>1020.04</td>\n",
       "      <td>59.08</td>\n",
       "      <td>444.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.11</td>\n",
       "      <td>39.40</td>\n",
       "      <td>1012.16</td>\n",
       "      <td>92.14</td>\n",
       "      <td>488.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.86</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1010.24</td>\n",
       "      <td>76.64</td>\n",
       "      <td>446.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.82</td>\n",
       "      <td>37.50</td>\n",
       "      <td>1009.23</td>\n",
       "      <td>96.62</td>\n",
       "      <td>473.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47835</th>\n",
       "      <td>15.12</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.80</td>\n",
       "      <td>72.93</td>\n",
       "      <td>462.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47836</th>\n",
       "      <td>33.41</td>\n",
       "      <td>77.95</td>\n",
       "      <td>1010.30</td>\n",
       "      <td>59.72</td>\n",
       "      <td>432.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47837</th>\n",
       "      <td>15.99</td>\n",
       "      <td>43.34</td>\n",
       "      <td>1014.20</td>\n",
       "      <td>78.66</td>\n",
       "      <td>465.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47838</th>\n",
       "      <td>17.65</td>\n",
       "      <td>59.87</td>\n",
       "      <td>1018.58</td>\n",
       "      <td>94.65</td>\n",
       "      <td>450.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47839</th>\n",
       "      <td>23.68</td>\n",
       "      <td>51.30</td>\n",
       "      <td>1011.86</td>\n",
       "      <td>71.24</td>\n",
       "      <td>451.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47840 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          AT      V       AP     RH      PE\n",
       "0      14.96  41.76  1024.07  73.17  463.26\n",
       "1      25.18  62.96  1020.04  59.08  444.37\n",
       "2       5.11  39.40  1012.16  92.14  488.56\n",
       "3      20.86  57.32  1010.24  76.64  446.48\n",
       "4      10.82  37.50  1009.23  96.62  473.90\n",
       "...      ...    ...      ...    ...     ...\n",
       "47835  15.12  48.92  1011.80  72.93  462.59\n",
       "47836  33.41  77.95  1010.30  59.72  432.90\n",
       "47837  15.99  43.34  1014.20  78.66  465.96\n",
       "47838  17.65  59.87  1018.58  94.65  450.93\n",
       "47839  23.68  51.30  1011.86  71.24  451.67\n",
       "\n",
       "[47840 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"C:/Users/manis/Downloads/data.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork (nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(4,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,1),\n",
    "        \n",
    "\n",
    "        )\n",
    "\n",
    "    def forward (self ,x_tensor):\n",
    "        logits=self.linear_relu_stack(x_tensor)\n",
    "        return logits\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47840, 4)\n",
      "(47840, 1)\n"
     ]
    }
   ],
   "source": [
    "x=df.drop(columns=['PE'])\n",
    "#y =df.iloc[:,-1]\n",
    "y=df.drop(columns=['AT','V','AP','RH'])\n",
    "#y=reshape(47840,1, keepdims=True)\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47840, 1])\n"
     ]
    }
   ],
   "source": [
    "x_tensor=torch.tensor(x.values,dtype=torch.float32)\n",
    "y_tensor=torch.tensor(y.values,dtype=torch.float32)\n",
    "#y_tensor.unsqueeze(1)\n",
    "#y_tensor.reshape(47840,1,keepdims)\n",
    "print(y_tensor.shape)\n",
    "dataset=TensorDataset(x_tensor,y_tensor)\n",
    "\n",
    "\n",
    "num_samples = len(dataset)\n",
    "indices = list(range(num_samples))\n",
    "split = int(num_samples * 0.8) \n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (dataloader , model  ,loss_fn , optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch ,(x,y)in enumerate(dataloader):\n",
    "        x,y=x.to(device), y.to(device)\n",
    "\n",
    "        pred=model(x)\n",
    "        loss=loss_fn(pred,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch %100==0:\n",
    "            loss,current=loss.item(),(batch+1)*len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (dataloader , model , loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches=len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_tensor, y_tensor in dataloader:\n",
    "            x_tensor, y_tensor = x_tensor.to(device), y_tensor.to(device)\n",
    "            pred = model(x_tensor)\n",
    "            test_loss += loss_fn(pred, y_tensor).item()\n",
    "            #correct += (pred.argmax(1) == y_tensor).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n  Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 202680.468750  [   32/47840]\n",
      "loss: 2067.227539  [ 3232/47840]\n",
      "loss: 206.368988  [ 6432/47840]\n",
      "loss: 266.881012  [ 9632/47840]\n",
      "loss: 282.799133  [12832/47840]\n",
      "loss: 174.371918  [16032/47840]\n",
      "loss: 284.479614  [19232/47840]\n",
      "loss: 269.796936  [22432/47840]\n",
      "loss: 175.035355  [25632/47840]\n",
      "loss: 213.281494  [28832/47840]\n",
      "loss: 156.343689  [32032/47840]\n",
      "loss: 148.635422  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 179.640121 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 191.883240  [   32/47840]\n",
      "loss: 178.507767  [ 3232/47840]\n",
      "loss: 129.523758  [ 6432/47840]\n",
      "loss: 144.981567  [ 9632/47840]\n",
      "loss: 131.701126  [12832/47840]\n",
      "loss: 92.085739  [16032/47840]\n",
      "loss: 123.488457  [19232/47840]\n",
      "loss: 78.671829  [22432/47840]\n",
      "loss: 94.856674  [25632/47840]\n",
      "loss: 68.101608  [28832/47840]\n",
      "loss: 100.797363  [32032/47840]\n",
      "loss: 57.236885  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 51.768526 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 58.073734  [   32/47840]\n",
      "loss: 58.176579  [ 3232/47840]\n",
      "loss: 42.360542  [ 6432/47840]\n",
      "loss: 54.680031  [ 9632/47840]\n",
      "loss: 36.961826  [12832/47840]\n",
      "loss: 42.686188  [16032/47840]\n",
      "loss: 41.023262  [19232/47840]\n",
      "loss: 42.394188  [22432/47840]\n",
      "loss: 18.526266  [25632/47840]\n",
      "loss: 24.227264  [28832/47840]\n",
      "loss: 45.766018  [32032/47840]\n",
      "loss: 28.324982  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 31.366821 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 81.285652  [   32/47840]\n",
      "loss: 25.882618  [ 3232/47840]\n",
      "loss: 31.697584  [ 6432/47840]\n",
      "loss: 36.634277  [ 9632/47840]\n",
      "loss: 37.782639  [12832/47840]\n",
      "loss: 28.417500  [16032/47840]\n",
      "loss: 24.999371  [19232/47840]\n",
      "loss: 33.820442  [22432/47840]\n",
      "loss: 35.082169  [25632/47840]\n",
      "loss: 24.315964  [28832/47840]\n",
      "loss: 30.085011  [32032/47840]\n",
      "loss: 28.626423  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 28.133751 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 37.690754  [   32/47840]\n",
      "loss: 26.915665  [ 3232/47840]\n",
      "loss: 23.843758  [ 6432/47840]\n",
      "loss: 33.086655  [ 9632/47840]\n",
      "loss: 51.747181  [12832/47840]\n",
      "loss: 26.913094  [16032/47840]\n",
      "loss: 22.732733  [19232/47840]\n",
      "loss: 39.735954  [22432/47840]\n",
      "loss: 23.934959  [25632/47840]\n",
      "loss: 28.444706  [28832/47840]\n",
      "loss: 25.952911  [32032/47840]\n",
      "loss: 22.783995  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 27.957809 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 41.408695  [   32/47840]\n",
      "loss: 13.587485  [ 3232/47840]\n",
      "loss: 25.025780  [ 6432/47840]\n",
      "loss: 24.352528  [ 9632/47840]\n",
      "loss: 63.106407  [12832/47840]\n",
      "loss: 15.092726  [16032/47840]\n",
      "loss: 13.399355  [19232/47840]\n",
      "loss: 20.569248  [22432/47840]\n",
      "loss: 26.144733  [25632/47840]\n",
      "loss: 32.427200  [28832/47840]\n",
      "loss: 24.461319  [32032/47840]\n",
      "loss: 77.168510  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 35.328388 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 33.681820  [   32/47840]\n",
      "loss: 30.807495  [ 3232/47840]\n",
      "loss: 31.024017  [ 6432/47840]\n",
      "loss: 30.361736  [ 9632/47840]\n",
      "loss: 77.026123  [12832/47840]\n",
      "loss: 23.101608  [16032/47840]\n",
      "loss: 30.218231  [19232/47840]\n",
      "loss: 35.386917  [22432/47840]\n",
      "loss: 27.729647  [25632/47840]\n",
      "loss: 29.132416  [28832/47840]\n",
      "loss: 18.331944  [32032/47840]\n",
      "loss: 32.513123  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 29.280281 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 21.817842  [   32/47840]\n",
      "loss: 32.100117  [ 3232/47840]\n",
      "loss: 85.280312  [ 6432/47840]\n",
      "loss: 30.126043  [ 9632/47840]\n",
      "loss: 41.075989  [12832/47840]\n",
      "loss: 15.874675  [16032/47840]\n",
      "loss: 21.250439  [19232/47840]\n",
      "loss: 19.504314  [22432/47840]\n",
      "loss: 17.265739  [25632/47840]\n",
      "loss: 27.679581  [28832/47840]\n",
      "loss: 35.517616  [32032/47840]\n",
      "loss: 24.325535  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 25.783200 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 24.447138  [   32/47840]\n",
      "loss: 14.874784  [ 3232/47840]\n",
      "loss: 33.999390  [ 6432/47840]\n",
      "loss: 31.014835  [ 9632/47840]\n",
      "loss: 64.155678  [12832/47840]\n",
      "loss: 23.350885  [16032/47840]\n",
      "loss: 34.212730  [19232/47840]\n",
      "loss: 32.225677  [22432/47840]\n",
      "loss: 22.673042  [25632/47840]\n",
      "loss: 60.885334  [28832/47840]\n",
      "loss: 26.155167  [32032/47840]\n",
      "loss: 35.060505  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 30.623526 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 17.803879  [   32/47840]\n",
      "loss: 32.089138  [ 3232/47840]\n",
      "loss: 27.665756  [ 6432/47840]\n",
      "loss: 23.147045  [ 9632/47840]\n",
      "loss: 18.363365  [12832/47840]\n",
      "loss: 33.690708  [16032/47840]\n",
      "loss: 29.895187  [19232/47840]\n",
      "loss: 24.710358  [22432/47840]\n",
      "loss: 97.508728  [25632/47840]\n",
      "loss: 21.451885  [28832/47840]\n",
      "loss: 24.078320  [32032/47840]\n",
      "loss: 30.761272  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 25.712780 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 26.736227  [   32/47840]\n",
      "loss: 17.956987  [ 3232/47840]\n",
      "loss: 22.455624  [ 6432/47840]\n",
      "loss: 20.326576  [ 9632/47840]\n",
      "loss: 32.673359  [12832/47840]\n",
      "loss: 20.845854  [16032/47840]\n",
      "loss: 21.172901  [19232/47840]\n",
      "loss: 18.931873  [22432/47840]\n",
      "loss: 14.418733  [25632/47840]\n",
      "loss: 22.405165  [28832/47840]\n",
      "loss: 24.900986  [32032/47840]\n",
      "loss: 25.549879  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 25.795319 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 21.031544  [   32/47840]\n",
      "loss: 23.322674  [ 3232/47840]\n",
      "loss: 14.689304  [ 6432/47840]\n",
      "loss: 46.717407  [ 9632/47840]\n",
      "loss: 36.848663  [12832/47840]\n",
      "loss: 30.227673  [16032/47840]\n",
      "loss: 21.684479  [19232/47840]\n",
      "loss: 55.648197  [22432/47840]\n",
      "loss: 29.399904  [25632/47840]\n",
      "loss: 23.537870  [28832/47840]\n",
      "loss: 20.892080  [32032/47840]\n",
      "loss: 54.149311  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 26.032033 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 26.231064  [   32/47840]\n",
      "loss: 34.190216  [ 3232/47840]\n",
      "loss: 25.603180  [ 6432/47840]\n",
      "loss: 24.152775  [ 9632/47840]\n",
      "loss: 34.836510  [12832/47840]\n",
      "loss: 19.353003  [16032/47840]\n",
      "loss: 25.358112  [19232/47840]\n",
      "loss: 41.436989  [22432/47840]\n",
      "loss: 30.112583  [25632/47840]\n",
      "loss: 24.732477  [28832/47840]\n",
      "loss: 21.005379  [32032/47840]\n",
      "loss: 37.400940  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 26.160039 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 24.368034  [   32/47840]\n",
      "loss: 21.528168  [ 3232/47840]\n",
      "loss: 25.816095  [ 6432/47840]\n",
      "loss: 25.539829  [ 9632/47840]\n",
      "loss: 34.964211  [12832/47840]\n",
      "loss: 21.288857  [16032/47840]\n",
      "loss: 64.001801  [19232/47840]\n",
      "loss: 21.090616  [22432/47840]\n",
      "loss: 31.868668  [25632/47840]\n",
      "loss: 27.452467  [28832/47840]\n",
      "loss: 39.326855  [32032/47840]\n",
      "loss: 17.205084  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 33.770395 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 24.670734  [   32/47840]\n",
      "loss: 25.162048  [ 3232/47840]\n",
      "loss: 25.682686  [ 6432/47840]\n",
      "loss: 27.576620  [ 9632/47840]\n",
      "loss: 78.516090  [12832/47840]\n",
      "loss: 30.309284  [16032/47840]\n",
      "loss: 31.153475  [19232/47840]\n",
      "loss: 30.578411  [22432/47840]\n",
      "loss: 36.855980  [25632/47840]\n",
      "loss: 21.638344  [28832/47840]\n",
      "loss: 17.140755  [32032/47840]\n",
      "loss: 27.982224  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 28.754328 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 20.713167  [   32/47840]\n",
      "loss: 17.359655  [ 3232/47840]\n",
      "loss: 18.431221  [ 6432/47840]\n",
      "loss: 25.274345  [ 9632/47840]\n",
      "loss: 22.531429  [12832/47840]\n",
      "loss: 24.469028  [16032/47840]\n",
      "loss: 23.345839  [19232/47840]\n",
      "loss: 22.025293  [22432/47840]\n",
      "loss: 30.701866  [25632/47840]\n",
      "loss: 37.681473  [28832/47840]\n",
      "loss: 52.703690  [32032/47840]\n",
      "loss: 35.987228  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 24.689370 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 23.403233  [   32/47840]\n",
      "loss: 32.857002  [ 3232/47840]\n",
      "loss: 32.084366  [ 6432/47840]\n",
      "loss: 36.365540  [ 9632/47840]\n",
      "loss: 23.235970  [12832/47840]\n",
      "loss: 25.415695  [16032/47840]\n",
      "loss: 22.788551  [19232/47840]\n",
      "loss: 33.279594  [22432/47840]\n",
      "loss: 59.469704  [25632/47840]\n",
      "loss: 23.024071  [28832/47840]\n",
      "loss: 42.481216  [32032/47840]\n",
      "loss: 24.974815  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 23.590823 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 21.707115  [   32/47840]\n",
      "loss: 19.924858  [ 3232/47840]\n",
      "loss: 33.450592  [ 6432/47840]\n",
      "loss: 16.990303  [ 9632/47840]\n",
      "loss: 20.623730  [12832/47840]\n",
      "loss: 14.661736  [16032/47840]\n",
      "loss: 14.351846  [19232/47840]\n",
      "loss: 15.655632  [22432/47840]\n",
      "loss: 20.599367  [25632/47840]\n",
      "loss: 19.613300  [28832/47840]\n",
      "loss: 22.939529  [32032/47840]\n",
      "loss: 22.203005  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 28.243434 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 36.737701  [   32/47840]\n",
      "loss: 31.924194  [ 3232/47840]\n",
      "loss: 24.283136  [ 6432/47840]\n",
      "loss: 31.002918  [ 9632/47840]\n",
      "loss: 18.115709  [12832/47840]\n",
      "loss: 43.117805  [16032/47840]\n",
      "loss: 31.402924  [19232/47840]\n",
      "loss: 21.586302  [22432/47840]\n",
      "loss: 22.956097  [25632/47840]\n",
      "loss: 21.550896  [28832/47840]\n",
      "loss: 26.340471  [32032/47840]\n",
      "loss: 27.017136  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 26.299486 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 27.439228  [   32/47840]\n",
      "loss: 25.166985  [ 3232/47840]\n",
      "loss: 27.712025  [ 6432/47840]\n",
      "loss: 28.209059  [ 9632/47840]\n",
      "loss: 19.825050  [12832/47840]\n",
      "loss: 17.709732  [16032/47840]\n",
      "loss: 31.829596  [19232/47840]\n",
      "loss: 24.613651  [22432/47840]\n",
      "loss: 21.969521  [25632/47840]\n",
      "loss: 37.157234  [28832/47840]\n",
      "loss: 29.282160  [32032/47840]\n",
      "loss: 31.288397  [35232/47840]\n",
      "Test Error: \n",
      "  Avg loss: 23.194867 \n",
      "\n",
      "Done ig\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(val_loader, model, loss_fn)\n",
    "print(\"Done ig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46326011e1ad7acc7b4f97a45114e7cb006fd16e22f399475266305dccb8d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
